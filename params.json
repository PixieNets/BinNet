{
  "name": "PixieNet",
  "tagline": "16-623 CMU Course project",
  "body": "####Team Members\r\nEsha Uboweja, Salvador Medina\r\n\r\n## Project Proposal\r\n\r\n### Summary\r\n\r\nWe aim to implement a binary convolutional neural network capable of being executed on an iOS device to run image classification on image frames captured by the device's camera.\r\n\r\n![Is that a bird?!](images/tasks_xkcd_1425.png)\r\n\r\nPerforming image detection on a mobile device ([source](http://xkcd.com/1425/))\r\n\r\n### Background\r\n\r\n* **Why is this a mobile device application project?**\r\n\r\nConvolutional neural networks (CNNs) work really well in tasks like object recognition and scene classification, and for building image embeddings (vectorial or feature representations) that can in turn be used in more complex tasks such as scene navigation, obstacle avoidance etc.\r\n\r\nPresently, there are many popular libraries and frameworks for deep learning such as Google's TensorFlow, Facebook's Torch, Caffe, Theano etc. There are iOS versions of some of these frameworks, such as TensorFlow and Torch. We examined an application published in the TensorFlow repository ([Object detection via the camera app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples)) wherein a pre-trained GoogleLeNet model (size approx. 50 MB) was able to perform object recognition on a video on our iPad Air 2 at 6.8 FPS. This is a decent speed because the application is able to perform real time object recognition on the iOS device. However, when we tried to change the application code to use a pre-trained VGG-16 Network (trained on 1001 classes of ImageNet dataset) instead, the application crashed due to the huge size of the weights file (approx. 553.5 MB). \r\n\r\n_Simply put, there is a problem of a huge memory footprint of many existing deep networks._\r\n\r\nThe ability of running such networks on mobile devices can help in research projects and applications. For instance, a high performance and accurate object/scene classifier can be used to guide the visually impaired, as it can give a coarse description of their surroundings. Further, a full working CNN on a mobile device will help in reducing the workload on server side computation (as how currently networks run in the cloud) as now the forward pass of a neural net can compute image features on the user's device.\r\n\r\n* **Computational speedups**\r\n\r\nWe aim to implement binary networks as described in [XNOR-Net](https://arxiv.org/abs/1603.05279). The key insight is that binary values occupy less memory than double/single-precision floating point values and hence if we use binary valued parameters and weights for the many layers and binary valued inputs, the memory footprint of the network will decrease. Further, operations like convolution can be implemented using binary operations that are faster than an _O(N^3)_ matrix multiplication. The authors of [XNOR-Net](https://arxiv.org/abs/1603.05279) claim that it is possible to compress networks like [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) to 7.4MB size with a small loss in prediction accuracy. \r\n\r\nThe implementation will therefore tap every possible source of speedup such as using fast matrix operation libraries like Armadillo, Accelerate framework, Eigen and very likely also tap the GPU using Metal in Objective-C. \r\nWe will present some evidence as to what we actually choose in our final implementation when we present benchmarks for our project (please see the [Goals & Deliverables](.###Goals-&-Deliverables) section).\r\n\r\n### The Challenge\r\n\r\nThe challenges involved in running a deep network on mobile device are two-fold, **memory** and **run-time**. As discussed in the [Background](.###Background) section, smaller memory footprint networks can actually run on the mobile device and faster operations involved in the forward pass at runtime can speedup the rate of processing input frames and running them at real time. \r\n\r\nWe would like to note the following:\r\n\r\n1. The key challenge here is to get the network running, not the accuracy of the network itself. This means that in our project, our main goal is to compress and run the network (even with just arbitrary values), and work on accuracy later.\r\n2. Our project focuses on _running_ the network and *not training* on the mobile device. This means that for accurate object recognition, we will take pre-trained network weights and use them at test time in the device application, similar to what is demonstrated in the [TensorFlow iOS camera application](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples).\r\n3. _Where do we get pre-trained weights for binary networks from?!_ : This is an interesting question because the authors of [XNOR-Net](https://arxiv.org/abs/1603.05279) highlight that they borrowed network architectures from [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) and [VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) and train the binary networks from scratch. This is our plan, but keep tuned in to see if come up with something more exciting!\r\n\r\n### Goals & Deliverables\r\n\r\n\r\n### Schedule",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}