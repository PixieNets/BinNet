<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>PixieNet by PixieNets</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>PixieNet</h1>
          <h2>16-623 CMU Course project</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/PixieNets/PixieNet/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/PixieNets/PixieNet/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/PixieNets/PixieNet" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h4>
<a id="team-members" class="anchor" href="#team-members" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Team Members</h4>

<p>Esha Uboweja, Salvador Medina</p>

<h2>
<a id="project-proposal" class="anchor" href="#project-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project Proposal</h2>

<h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>We aim to implement a binary convolutional neural network capable of being executed on an iOS device to run image classification on image frames captured by the device's camera.</p>

<p><img src="images/tasks_xkcd_1425.png" alt="Is that a bird?!"></p>

<p>Performing image detection on a mobile device (<a href="http://xkcd.com/1425/">source</a>)</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<ul>
<li><strong>Why is this a mobile device application project?</strong></li>
</ul>

<p>Convolutional neural networks (CNNs) work really well in tasks like object recognition and scene classification, and for building image embeddings (vectorial or feature representations) that can in turn be used in more complex tasks such as scene navigation, obstacle avoidance etc.</p>

<p>Presently, there are many popular libraries and frameworks for deep learning such as Google's TensorFlow, Facebook's Torch, Caffe, Theano etc. There are iOS versions of some of these frameworks, such as TensorFlow and Torch. We examined an application published in the TensorFlow repository (<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples">Object detection via the camera app</a>) wherein a pre-trained <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf">GoogleLeNet</a> model (size approx. 50 MB) was able to perform object recognition on a video on our iPad Air 2 at 6.8 FPS. This is a decent speed because the application is able to perform real time object recognition on the iOS device. However, when we tried to change the application code to use a pre-trained VGG-16 Network (trained on 1001 classes of ImageNet dataset) instead, the application crashed due to the huge size of the weights file (approx. 553.5 MB). </p>

<p><em>Simply put, there is a problem of a huge memory footprint of many existing deep networks.</em></p>

<p>The ability of running such networks on mobile devices can help in research projects and applications. For instance, a high performance and accurate object/scene classifier can be used to guide the visually impaired, as it can give a coarse description of their surroundings. Further, a full working CNN on a mobile device will help in reducing the workload on server side computation (as how currently networks run in the cloud) as now the forward pass of a neural net can compute image features on the user's device.</p>

<ul>
<li><strong>Computational speedups</strong></li>
</ul>

<p>We aim to implement binary networks as described in <a href="https://arxiv.org/abs/1603.05279">XNOR-Net</a>. The key insight is that binary values occupy less memory than double/single-precision floating point values and hence if we use binary valued parameters and weights for the many layers and binary valued inputs, the memory footprint of the network will decrease. Further, operations like convolution can be implemented using binary operations that are faster than an <em>O(N^3)</em> matrix multiplication. The authors of <a href="https://arxiv.org/abs/1603.05279">XNOR-Net</a> claim that it is possible to compress networks like <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> to 7.4MB size with a small loss in prediction accuracy. </p>

<p>The implementation will therefore tap every possible source of speedup such as using fast matrix operation libraries like Armadillo, Accelerate framework, Eigen and very likely also tap the GPU using Metal in Objective-C. 
We will present some evidence as to what we actually choose in our final implementation when we present benchmarks for our project (please see the <a href=".###Goals-&amp;-Deliverables">Goals &amp; Deliverables</a> section).</p>

<h3>
<a id="the-challenge" class="anchor" href="#the-challenge" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Challenge</h3>

<p>The challenges involved in running a deep network on mobile device are two-fold, <strong>memory</strong> and <strong>run-time</strong>. As discussed in the <a href=".###Background">Background</a> section, smaller memory footprint networks can actually run on the mobile device and faster operations involved in the forward pass at runtime can speedup the rate of processing input frames and running them at real time. </p>

<p>We would like to note the following:</p>

<ol>
<li>The key challenge here is to get the network running, not the accuracy of the network itself. This means that in our project, our main goal is to compress and run the network (even with just arbitrary values), and work on accuracy later.</li>
<li>Our project focuses on <em>running</em> the network and <em>not training</em> on the mobile device. This means that for accurate object recognition, we will take pre-trained network weights and use them at test time in the device application, similar to what is demonstrated in the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples">TensorFlow iOS camera application</a>.</li>
<li>
<em>Where do we get pre-trained weights for binary networks from?!</em> : This is an interesting question because the authors of <a href="https://arxiv.org/abs/1603.05279">XNOR-Net</a> highlight that they borrowed network architectures from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> and <a href="http://www.robots.ox.ac.uk/%7Evgg/research/very_deep/">VGGNet</a> and train the binary networks from scratch. This is our plan too, when we work on improving the accuracy of the network for object/scene recognition.</li>
</ol>

<h3>
<a id="goals--deliverables" class="anchor" href="#goals--deliverables" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goals &amp; Deliverables</h3>

<ul>
<li>We <em>plan to achieve</em> an implementation of a fully end-to-end binarized network framework for iOS that can run a network like <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> on the device.</li>
<li>We <em>hope to achive</em> the full deployment of an application for iOS with a compact model of pre-trained weights, capable of classifying objects and/or scenes that are captured by the camera.</li>
</ul>

<p><strong>Evaluating our delivarables</strong></p>

<ul>
<li><p>We plan to present benchmarks for our framework as we implement and optimize it. The benchmarks will show the size of the pre-trained model that we compress using a binary network and the speed of the computation for a forward neural network pass for object recognition on a live video.</p></li>
<li><p>A baseline is already provided to us by the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples">TensorFlow iOS camera application</a> which uses a pre-trained <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf">GoogleLeNet</a> model (size approx. 50 MB) was able to perform object recognition on a video on our iPad Air 2 at 6.8 FPS. So one measure of success would be to beat this benchmark on the GoogleLeNet architecture.</p></li>
</ul>

<p><img src="images/einstein_cartoon1-full-100x125.jpg" alt="Realism in life">
(<a href="http://blogs.agu.org/wildwildscience/files/2009/09/einstein_cartoon1-full-336x420.jpg">source</a>)</p>

<p>** "Let's be real folks, you have only 4 weeks!" **</p>

<h3>
<a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Schedule</h3>
        </section>

        <footer>
          PixieNet is maintained by <a href="https://github.com/PixieNets">PixieNets</a><br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>
